import { Injectable, Logger, NotFoundException } from '@nestjs/common';
import { SupabaseService } from '../supabase/supabase.service';
import { LlamaService } from '../llama/llama.service';
import { VisionPipelineService } from '../vision-pipeline/vision-pipeline.service';

@Injectable()
export class RagService {
  private readonly logger = new Logger(RagService.name);

  constructor(
    private readonly supabaseService: SupabaseService,
    private readonly llamaService: LlamaService,
    private readonly visionPipelineService: VisionPipelineService,
  ) {}

  /**
   * Query the RAG system with user input
   *
   * @param query The user's query
   * @param conversationId Optional ID for tracking conversation context
   * @param conversationContext Optional previous messages for context
   * @returns RAG response with answer and source references
   */
  async queryRag(
    query: string,
    conversationId?: string,
    conversationContext?: any[],
  ) {
    this.logger.log(`Processing RAG query: ${query}`);

    try {
      // TODO: Implement RAG query pipeline
      // 1. Extract keywords from query
      // 2. Retrieve relevant documents from Supabase (could be from different sources)
      // 3. Combine documents with query into a prompt
      // 4. Generate response using LLM
      // 5. Track source references for context

      // Generate tracking ID for this response
      const responseId = `resp_${Date.now()}`;

      // Step 1: Extract keywords from query
      const keywords = this.extractKeywords(query);
      this.logger.log(`Extracted keywords: ${keywords.join(', ')}`);

      // Step 2: Retrieve relevant documents
      // TODO: Implement retrieval from Supabase
      // This should include:
      // - Text content from Notion pages
      // - Code blocks
      // - Image descriptions (from vision pipeline)
      // - Chart descriptions

      // For scaffolding purposes, we'll use placeholder data
      const retrievedDocuments = await this.retrieveRelevantDocuments(keywords);
      
      // Step 3: Build prompt with context
      const prompt = this.buildPromptWithContext(
        query,
        retrievedDocuments,
        conversationContext,
      );
      
      // Step 4: Generate response using LLM
      // TODO: Implement actual LLM call
      // const response = await this.llamaService.getCompletion(prompt);
      const response = await this.llamaService.getCompletion(prompt, {
        maxTokens: 512,
      });
      // For scaffolding purposes, we'll use a placeholder response
      // const response = {
      //   text: `This is a placeholder response for: "${query}". In a real implementation, this would be generated by the RAG system using retrieved documents and an LLM.`,
      // };

      // Step 5: Track source references
      const sourceReferences = this.prepareSourceReferences(
        retrievedDocuments,
        responseId,
      );

      // TODO: Store the query, response, and source references in Supabase for history
      await this.supabaseService.client
      .from('rag_responses')
      .insert({
        id: responseId,
        query,
        response: response.text,
        created_at: new Date().toISOString(),
      });

    await this.supabaseService.client
      .from('rag_source_references')
      .insert(
        sourceReferences.map((src) => ({
          id: src.id,
          response_id: responseId,
          page_path: src.pagePath,
          page_title: src.pageTitle,
          content: src.content,
          type: src.type,
          url: src.url,
          metadata: src,
        })),
      );

      return {
        response: response.text,
        sourceReferences,
        query,
        responseId,
        processedAt: new Date().toISOString(),
      };
    } catch (error) {
      this.logger.error(
        `Error processing RAG query: ${error.message}`,
        error.stack,
      );
      throw error;
    }
  }

  /**
   * Extract keywords from a query
   */
  private extractKeywords(query: string): string[] {
    // TODO: Implement keyword extraction
    // This could use NLP techniques or simply filter common words

    // For scaffolding purposes, we'll just split and filter
    const stopWords = [
      'the',
      'a',
      'an',
      'in',
      'on',
      'at',
      'to',
      'for',
      'with',
      'by',
    ];
    return query
      .toLowerCase()
      .split(/\s+/)
      .filter((word) => word.length > 2 && !stopWords.includes(word));
  }

  /**
   * Retrieve relevant documents for the given keywords
   */
  private async retrieveRelevantDocuments(keywords: string[]): Promise<any[]> {
    // 1) Embed the combined keywords
    const textToEmbed = keywords.join(' ');
    const embedding = await this.llamaService.getEmbedding(textToEmbed);
  
    // 2) Call Supabase RPC to do a vector‐search
    const { data: rawDocs, error } = await this.supabaseService.client
      .rpc('match_documents', {
        query_embedding: embedding,
        match_count: 5,
        similarity_threshold: 0.75,
      });
  
    if (error) {
      this.logger.error('Supabase vector search failed', error);
      throw new Error(error.message);
    }
    const docs = (rawDocs as any[]);
  
    // Normalize fields to internal doc shape (bc some field names are different)
    const normalized = docs.map((doc, idx) => ({
      id: doc.id,
      type: doc.type,
      content: doc.content,
      title: doc.title,
      source: doc.source,
      pageId: doc.page_id,
      pageTitle: doc.page_title,
      pagePath: doc.page_path,
      imageUrl: doc.image_url,
      relevanceScore: doc.similarity,
      language: doc.language,
      blockId: doc.block_id,
    }));
  
    // Enrich image docs w VisionPipeline
    const enriched = await Promise.all(
      normalized.map(async (doc) => {
        if (doc.type === 'image' && doc.imageUrl) {
          const processed = await this.visionPipelineService.processPageWithImages(
            doc.pageId,
            doc.content,
            [doc.imageUrl],
          );
          return {
            ...doc,
            content: processed.processed_content,
            // include any image_references
            imageReferences: processed.image_references,
          };
        }
        return doc;
      }),
    );
  
    return enriched;
  }

  /**
   * Build a prompt with context for the LLM
   */
  private buildPromptWithContext(
    query: string,
    documents: any[],
    conversationContext?: any[],
  ): string {
    // Header / system instruction
    let prompt = `You are an Engineering Wiki Assistant. 
Use only the provided context to answer the user’s question. 
If the context does not contain the answer, say you don’t have enough information.\n\n`;

    // Document context: numbered entries with titles and content
    if (documents.length > 0) {
      prompt += 'Context sources:\n';
      documents.forEach((doc, i) => {
        prompt += `\n[${i + 1}] ${doc.title}\n`;
        prompt += doc.content.trim() + '\n';
        // if there are image references, list their markers
        if (Array.isArray(doc.imageReferences)) {
          doc.imageReferences.forEach((imgRef) => {
            prompt += `  - [Image:${imgRef.image_id}] ${imgRef.marker}\n`;
          });
        }
      });
      prompt += '\n';
    }

    // Previous conversation (if any)
    if (conversationContext && conversationContext.length) {
      prompt += 'Conversation history:\n';
      conversationContext.forEach((m) => {
        const speaker = m.role === 'user' ? 'User' : 'Assistant';
        prompt += `${speaker}: ${m.content.trim()}\n`;
      });
      prompt += '\n';
    }

    // User question
    prompt += `User question: ${query.trim()}\n\n`;
    prompt += 'Answer below. Cite context sources by number: “[1]”, “[2]”, etc.\n';

    return prompt.trim();
  }

  /**
   * Prepare source references for the frontend
   */
  private prepareSourceReferences(documents: any[], responseId: string): any[] {
    // Convert the documents to the format expected by the frontend
    return documents.map((doc, index) => {
      // Generate a reference ID that includes the response ID for tracking
      const referenceId = `${responseId}_src${index}`;

      return {
        id: referenceId,
        title: doc.title,
        content: doc.content,
        type: doc.type,
        confidence: doc.relevanceScore,
        pageTitle: doc.pageTitle,
        pagePath: doc.pagePath,
        imageUrl: doc.imageUrl,
        language: doc.language,
        referenceId,
        url:
          doc.source === 'notion'
            ? `https://notion.so/${doc.pageId}`
            : undefined,
        position: {
          blockId: doc.blockId,
        },
      };
    });
  }

  /**
   * Get a specific source reference by ID
   */
  async getSourceReference(referenceId: string) {
    this.logger.log(`Fetching source reference: ${referenceId}`);
  
    try {
      // Query the rag_source_references table by ID
      const { data, error } = await this.supabaseService.client
        .from('rag_source_references')
        .select(`
          id,
          title,
          content,
          type,
          language,
          page_title,
          page_path,
          url,
          metadata
        `)
        .eq('id', referenceId)
        .single();
  
      if (error) {
        // 116 = “no rows returned”
        if ((error as any).code === 'PGRST116') {
          throw new NotFoundException(`Source reference not found: ${referenceId}`);
        }
        this.logger.error(`Supabase error fetching source reference`, error);
        throw new Error(error.message);
      }
  
      if (!data) {
        throw new NotFoundException(`Source reference not found: ${referenceId}`);
      }
  
      // Merge stored metadata with fields in response
      return {
        id: data.id,
        title: data.title,
        content: data.content,
        type: data.type,
        language: data.language,
        pageTitle: data.page_title,
        pagePath: data.page_path,
        url: data.url,
        ...data.metadata,   // any extra metadata we stored
      };
    } catch (err) {
      this.logger.error(`Error fetching source reference: ${err.message}`, err.stack);
      throw err;
    }
  }
  /**
   * Get all source references for a response
   */
  async getSourceReferencesForResponse(responseId: string): Promise<any[]> {
    this.logger.log(`Fetching sources for response: ${responseId}`);
  
    try {
      // Query Supabase for all source refs linked to this response
      const { data, error } = await this.supabaseService.client
        .from('rag_source_references')
        .select(`
          id,
          title,
          content,
          type,
          language,
          page_title,
          page_path,
          url,
          metadata
        `)
        .eq('response_id', responseId);
  
      if (error) {
        this.logger.error(
          `Supabase error fetching source references for response ${responseId}`,
          error
        );
        throw new Error(error.message);
      }
  
      if (!data || data.length === 0) {
        this.logger.warn(`No sources found for response: ${responseId}`);
        return [];
      }
  
      // Map DB rows into frontend‐friendly shape
      return data.map((row) => ({
        id: row.id,
        title: row.title,
        content: row.content,
        type: row.type,
        language: row.language,
        pageTitle: row.page_title,
        pagePath: row.page_path,
        url: row.url,
        ...row.metadata,        // spread any extra fields, e.g. confidence, imageUrl, etc.
      }));
    } catch (error) {
      this.logger.error(
        `Error fetching sources for response: ${error.message}`,
        error.stack
      );
      throw error;
    }
  }
}
